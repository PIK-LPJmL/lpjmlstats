<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>benchmark</title>

<script src="data:application/javascript;base64,Ly8gUGFuZG9jIDIuOSBhZGRzIGF0dHJpYnV0ZXMgb24gYm90aCBoZWFkZXIgYW5kIGRpdi4gV2UgcmVtb3ZlIHRoZSBmb3JtZXIgKHRvCi8vIGJlIGNvbXBhdGlibGUgd2l0aCB0aGUgYmVoYXZpb3Igb2YgUGFuZG9jIDwgMi44KS4KZG9jdW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignRE9NQ29udGVudExvYWRlZCcsIGZ1bmN0aW9uKGUpIHsKICB2YXIgaHMgPSBkb2N1bWVudC5xdWVyeVNlbGVjdG9yQWxsKCJkaXYuc2VjdGlvbltjbGFzcyo9J2xldmVsJ10gPiA6Zmlyc3QtY2hpbGQiKTsKICB2YXIgaSwgaCwgYTsKICBmb3IgKGkgPSAwOyBpIDwgaHMubGVuZ3RoOyBpKyspIHsKICAgIGggPSBoc1tpXTsKICAgIGlmICghL15oWzEtNl0kL2kudGVzdChoLnRhZ05hbWUpKSBjb250aW51ZTsgIC8vIGl0IHNob3VsZCBiZSBhIGhlYWRlciBoMS1oNgogICAgYSA9IGguYXR0cmlidXRlczsKICAgIHdoaWxlIChhLmxlbmd0aCA+IDApIGgucmVtb3ZlQXR0cmlidXRlKGFbMF0ubmFtZSk7CiAgfQp9KTsK"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>



<style type="text/css">
  code {
    white-space: pre;
  }
  .sourceCode {
    overflow: visible;
  }
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<link rel="stylesheet" href="data:text/css,body%20%7B%0Abackground%2Dcolor%3A%20%23fff%3B%0Amargin%3A%201em%20auto%3B%0Amax%2Dwidth%3A%20700px%3B%0Aoverflow%3A%20visible%3B%0Apadding%2Dleft%3A%202em%3B%0Apadding%2Dright%3A%202em%3B%0Afont%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0Afont%2Dsize%3A%2014px%3B%0Aline%2Dheight%3A%201%2E35%3B%0A%7D%0A%23TOC%20%7B%0Aclear%3A%20both%3B%0Amargin%3A%200%200%2010px%2010px%3B%0Apadding%3A%204px%3B%0Awidth%3A%20400px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Aborder%2Dradius%3A%205px%3B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Afont%2Dsize%3A%2013px%3B%0Aline%2Dheight%3A%201%2E3%3B%0A%7D%0A%23TOC%20%2Etoctitle%20%7B%0Afont%2Dweight%3A%20bold%3B%0Afont%2Dsize%3A%2015px%3B%0Amargin%2Dleft%3A%205px%3B%0A%7D%0A%23TOC%20ul%20%7B%0Apadding%2Dleft%3A%2040px%3B%0Amargin%2Dleft%3A%20%2D1%2E5em%3B%0Amargin%2Dtop%3A%205px%3B%0Amargin%2Dbottom%3A%205px%3B%0A%7D%0A%23TOC%20ul%20ul%20%7B%0Amargin%2Dleft%3A%20%2D2em%3B%0A%7D%0A%23TOC%20li%20%7B%0Aline%2Dheight%3A%2016px%3B%0A%7D%0Atable%20%7B%0Amargin%3A%201em%20auto%3B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dcolor%3A%20%23DDDDDD%3B%0Aborder%2Dstyle%3A%20outset%3B%0Aborder%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0Aborder%2Dwidth%3A%202px%3B%0Apadding%3A%205px%3B%0Aborder%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0Aborder%2Dwidth%3A%201px%3B%0Aborder%2Dstyle%3A%20inset%3B%0Aline%2Dheight%3A%2018px%3B%0Apadding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0Aborder%2Dleft%2Dstyle%3A%20none%3B%0Aborder%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Ap%20%7B%0Amargin%3A%200%2E5em%200%3B%0A%7D%0Ablockquote%20%7B%0Abackground%2Dcolor%3A%20%23f6f6f6%3B%0Apadding%3A%200%2E25em%200%2E75em%3B%0A%7D%0Ahr%20%7B%0Aborder%2Dstyle%3A%20solid%3B%0Aborder%3A%20none%3B%0Aborder%2Dtop%3A%201px%20solid%20%23777%3B%0Amargin%3A%2028px%200%3B%0A%7D%0Adl%20%7B%0Amargin%2Dleft%3A%200%3B%0A%7D%0Adl%20dd%20%7B%0Amargin%2Dbottom%3A%2013px%3B%0Amargin%2Dleft%3A%2013px%3B%0A%7D%0Adl%20dt%20%7B%0Afont%2Dweight%3A%20bold%3B%0A%7D%0Aul%20%7B%0Amargin%2Dtop%3A%200%3B%0A%7D%0Aul%20li%20%7B%0Alist%2Dstyle%3A%20circle%20outside%3B%0A%7D%0Aul%20ul%20%7B%0Amargin%2Dbottom%3A%200%3B%0A%7D%0Apre%2C%20code%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0Aborder%2Dradius%3A%203px%3B%0Acolor%3A%20%23333%3B%0Awhite%2Dspace%3A%20pre%2Dwrap%3B%20%0A%7D%0Apre%20%7B%0Aborder%2Dradius%3A%203px%3B%0Amargin%3A%205px%200px%2010px%200px%3B%0Apadding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0Abackground%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0Acode%20%7B%0Afont%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0Afont%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0Apadding%3A%202px%200px%3B%0A%7D%0Adiv%2Efigure%20%7B%0Atext%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0Abackground%2Dcolor%3A%20%23FFFFFF%3B%0Apadding%3A%202px%3B%0Aborder%3A%201px%20solid%20%23DDDDDD%3B%0Aborder%2Dradius%3A%203px%3B%0Aborder%3A%201px%20solid%20%23CCCCCC%3B%0Amargin%3A%200%205px%3B%0A%7D%0Ah1%20%7B%0Amargin%2Dtop%3A%200%3B%0Afont%2Dsize%3A%2035px%3B%0Aline%2Dheight%3A%2040px%3B%0A%7D%0Ah2%20%7B%0Aborder%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Apadding%2Dbottom%3A%202px%3B%0Afont%2Dsize%3A%20145%25%3B%0A%7D%0Ah3%20%7B%0Aborder%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0Apadding%2Dtop%3A%2010px%3B%0Afont%2Dsize%3A%20120%25%3B%0A%7D%0Ah4%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0Amargin%2Dleft%3A%208px%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Ah5%2C%20h6%20%7B%0Aborder%2Dbottom%3A%201px%20solid%20%23ccc%3B%0Afont%2Dsize%3A%20105%25%3B%0A%7D%0Aa%20%7B%0Acolor%3A%20%230033dd%3B%0Atext%2Ddecoration%3A%20none%3B%0A%7D%0Aa%3Ahover%20%7B%0Acolor%3A%20%236666ff%3B%20%7D%0Aa%3Avisited%20%7B%0Acolor%3A%20%23800080%3B%20%7D%0Aa%3Avisited%3Ahover%20%7B%0Acolor%3A%20%23BB00BB%3B%20%7D%0Aa%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0Aa%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0Atext%2Ddecoration%3A%20underline%3B%20%7D%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%0A" type="text/css" />




</head>

<body>




<h1 class="title toc-ignore">benchmark</h1>



<p>This vignette guides you through using the benchmark function of lpjmlstats in a typical LPJmL development workflow.</p>
<p>Say you have been working on a change in the LPJmL setup, tweaking some settings in the config or making changes to the source code. A global run with the new setup has been performed and the output was sucessfully written to the output directory. At this point, the natural question arises: ‘How have the modifications affected the results?’ The benchmarking system helps find an answer by<br />
systematically comparing the outputs of the modified, currently “under test,” LPJmL setup against the outputs of a stable “baseline” setup.</p>
<p>Results from different baseline LPJmL versions can be found in the <code>p/projects/lpjml/benchmark_run_outputs/</code> directory on the cluster.</p>
<p>This package is a component of the PIAM cluster module. It is recommended to load this module via <code>module load piam</code> before starting the R session, ensuring that all necessary packages are loaded.</p>
<div id="using-the-benchmark-function" class="section level1">
<h1>Using the benchmark function</h1>
<p>In this vignette we will evaluate how the results of LPJmL changed, going from version 5.7.1 to version 5.8.6.</p>
<p>Start by loading the package.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(lpjmlstats)</span></code></pre></div>
<p>For a quick completion of the tutorial, we will only look at the first five years of all LPJmL outputs, which significantly reduces computation time. If you prefer a full comparison, skip the following modification to the package settings.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set_lpjmlstats_settings</span>(<span class="at">year_subset =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span></code></pre></div>
<p>Remember to restore the default settings after the completion of the tutorial with <code>set_lpjmlstats_settings(year_subset = NULL)</code>.</p>
<p>The basic usage of the benchmarking function is simple. Enter the paths of the under test and baseline output directories, along with some meta-information.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>benchmark_obj <span class="ot">&lt;-</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">benchmark</span>(</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">baseline_dir =</span> <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.7.1/output&quot;</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># it is recommended to use absolute paths</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="at">under_test_dirs =</span> <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.8.6/output&quot;</span>,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="at">author =</span> <span class="st">&quot;David&quot;</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">settings =</span> <span class="fu">list</span>(<span class="at">vegc =</span> <span class="fu">c</span>(GlobAvgAnnAvgTimeseries)),</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>    <span class="at">description =</span> <span class="st">&quot;Evaluate the changes from version 5.7.1 to 5.8.6&quot;</span>,</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    <span class="at">output_file =</span> <span class="st">&quot;benchmark_vignette.pdf&quot;</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  )</span></code></pre></div>
<p>This will create a PDF report in the working directory and stores the numerical results in the <code>benchmark_obj</code> object. The process usually takes several minutes. It will summarize and compare the typically most relevant LPJmL outputs. Note that the console output highlights the currently processed output variable in blue. After the core numerical benchmarking is completed, it will start to generate the PDF report, which visualizes the numerical results in different plots and tables.</p>
<p>Use <code>?benchmark</code> to see how the output directory for the PDF report as well as other function arguments can be defined. In the details section of the function documentation you can also see the requirements for the function to work properly.</p>
<p>One of the most important other arguments that can be passed to the benchmark function is the <code>settings</code> list. It defines the full protocol of the benchmarking process, including which outputs are considered, how they are processed, compared and visualized, and what the order of the processing and output PDF is. If no settings are passed, the default as given in the <code>R\default_settings.R</code> package file is used. Here is an excerpt of that file:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>default_settings <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>  <span class="at">vegc  =</span> <span class="fu">c</span>(GlobSumTimeAvgTable, GlobSumAnnAvgTimeseries, TimeAvgMap),</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>  <span class="at">soilc =</span> <span class="fu">c</span>(GlobSumTimeAvgTable, GlobSumAnnAvgTimeseries, TimeAvgMap),</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>  <span class="at">litc =</span> <span class="fu">c</span>(GlobSumTimeAvgTable, GlobSumAnnAvgTimeseries, TimeAvgMap),</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>  ...</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>The settings list reads as follows: Each line corresponds to an output variable. The left hand side of the equality sign defines <em>which output is processed</em> and the right hand side defines <em>how this output is processed</em>.</p>
<p>More specifically the right hand side defines which metrics are used for the evaluation of the output. A metric is a generic pipeline to summarize, compare and visualize output data. It is generic because the same metric can be applied to different outputs. The metrics are the cornerstone of the benchmarking process. See <code>?Metric</code> for technical details.</p>
<p>The metric <code>GlobSumTimeAvgTable</code> globally sums up all cellular values weighted by reference area, and then averages over time. The resulting scalar values are compared and displayed in a table. The metric <code>GlobSumAnnAvgTimeseries</code> also computes a weighted sum, but only averages over each year (if the output is not already given anually). The resulting time series are displayed in a plot. Finally, <code>TimeAvgMap</code> only averages over time, and plots the differences from under-test to baseline as maps. Type <code>?GlobSumTimeAvgTable</code> for details.</p>
<p>Often it is needed to change these settings, for example when new outputs should be evaluated. Read the <a href="benchmark-change-settings.html">benchmark-change-settings</a> vignette for a tutorial.</p>
<p>Benchmarking multiple under test runs against a baseline is also possible. For example, it could be interesting to compare the 5.8.1 version with several older versions.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">benchmark</span>(</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.8.1/output&quot;</span>,</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.7.7/output&quot;</span>,</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>    <span class="co"># note that older versions than 5.7.1 can at the moment not be benchmarked due</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="co"># to missing terr_area output</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.7.1/output&quot;</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>  ),</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">author =</span> <span class="st">&quot;David&quot;</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">description =</span> <span class="st">&quot;Check how newer lpjml versions compare to older versions&quot;</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">output_file =</span> <span class="st">&quot;benchmark_vignette_multiple_ut.pdf&quot;</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>)</span></code></pre></div>
<p>Comparing more than two under test to the baseline may result in display problems in the PDF report.</p>
</div>
<div id="working-with-the-benchmark-results" class="section level1">
<h1>Working with the benchmark results</h1>
<div id="the-pdf-report" class="section level2">
<h2>The PDF report</h2>
<p>The PDF report offers a visualization of the numerical benchmarking results. It is structured by the different metrics used in the settings. Each metric has its own section where the result from all outputs using this metric are displayed.</p>
<p>The front page shows some meta information on the benchmarking. Of particular importance is the simulation table, that maps short simulation identifiers to the runs considered in the benchmarking. Usually these identifier are abbreviations of the simulation names, as defined in the lpjml config. If these names are not unique, abbreviations of the simulation paths are used. The identifiers are employed because the simulation names and paths are often quite long, making it more convenient to use short abbreviations to refer to the different runs in the report, especially when comparing the baseline with multiple under test runs.</p>
</div>
<div id="the-benchmark-data-object" class="section level2">
<h2>The benchmark data object</h2>
<p>The benchmarking function also outputs its numerical results as a benchmark data object. Accessing this data is a bit technical, so skip this section if you are only interested in the PDF report.</p>
<p>The benchmark object behaves similarly to a nested R list. The top layer is again structured by the different metrics. The data that is generated by a metric is stored in its var_grp_list attribute. It contains the results for all the variables that the metric evaluated as a list of so called variable groups.<br />
Each variable group consists of the processed baseline and (possibly multiple) under test outputs of that variable, as well as the comparisons of all under test outputs against the baseline. See <code>?Metric</code> for details on how the metrics organize their processed data.</p>
<p>Say we are interested in how the time series of global vegetation carbon has changed in the newer version. The data from the baseline run can be accessed via</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>vegc_old_ts <span class="ot">&lt;-</span> benchmark_obj<span class="sc">$</span>GlobSumAnnAvgTimeseries<span class="sc">$</span>var_grp_list<span class="sc">$</span>vegc<span class="sc">$</span>baseline</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>vegc_old_ts</span></code></pre></div>
<p>Note that this is an LPJmLData object of <code>lpjmlkit</code>. Evaluating an LPJmLData object in the R Console will provide you with a lot of useful meta data, about the origin and processing history of the object. Note that subset is <code>TRUE</code> and space aggregation is <code>weighted_sum</code>, which reflects the processing steps done by the benchmarking function.</p>
<p>You will encounter several elements of the <code>lpjmlkit</code> package in <code>lpjmlstats</code> - in fact <code>lpjmlstats</code> completly builds upon <code>lpjmlkit</code>.</p>
<p>This also means that we can employ the functionality of this latter package to work with the benchmark results. For example, to output the first 5 years of <code>vegc_old_ts</code> we use</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">subset</span>(vegc_old_ts, <span class="at">time =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)<span class="sc">$</span>data</span></code></pre></div>
<p>For under test results the simulation identifiers are used to access the data. This is needed to differentiate the under test outputs if multiple runs are benchmarked against a baseline run.</p>
<p>The simulation table that relates the identifiers to simulation names, as well as other meta data can be accessed via:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="fu">get_benchmark_meta_data</span>(benchmark_obj)</span></code></pre></div>
<p>Note that the simulation with the new version has the identifier <code>5.8.</code>. Thus, to access the new vegc time series of the new LPJmL version we use</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>vegc_new_ts <span class="ot">&lt;-</span> benchmark_obj<span class="sc">$</span>GlobSumAnnAvgTimeseries<span class="sc">$</span>var_grp_list<span class="sc">$</span>vegc<span class="sc">$</span>under_test<span class="sc">$</span><span class="st">`</span><span class="at">5.8.</span><span class="st">`</span></span></code></pre></div>
<p>To check the difference between the two time series we use</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="fu">subset</span>(vegc_new_ts, <span class="at">time =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)<span class="sc">$</span>data <span class="sc">-</span> <span class="fu">subset</span>(vegc_old_ts, <span class="at">time =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)<span class="sc">$</span>data</span></code></pre></div>
<p>Observe that the newer version produced less vegetation carbon in the first five years.</p>
<p>The data from the benchmarking object in combination with the functionality of lpjmlkit is also useful to make comparisons to literature data. Say we have a reference dataset of global soil nitrogen given as a terra object. Again the tools from lpjmlkit come in handy to convert the benchmark data to terra:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>soiln_lpjmldata <span class="ot">&lt;-</span> benchmark_obj<span class="sc">$</span>TimeAvgMap<span class="sc">$</span>var_grp_list<span class="sc">$</span>soiln<span class="sc">$</span>under_test<span class="sc">$</span><span class="st">`</span><span class="at">5.8.</span><span class="st">`</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>soiln_terra <span class="ot">&lt;-</span> lpjmlkit<span class="sc">::</span><span class="fu">as_terra</span>(soiln_lpjmldata)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>soiln_terra</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># read in validation data terra</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="co"># ...</span></span></code></pre></div>
<p>Another convenient feature of the benchmark data object is that all plot functions used for the report generation can also be called individually from here. For example the table of the <code>GlobSumTimeAvgTable</code> metric can be generated via</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>benchmark_obj<span class="sc">$</span>GlobSumTimeAvgTable<span class="sc">$</span><span class="fu">plot</span>()</span></code></pre></div>
<p>This enables a terminal-only benchmarking without generating plots or reports. See <code>?benchmark</code> for how to skip the PDF report generation in the benchmarking.</p>
<p>Sometimes individual plots of the benchmarking may be needed in another context. The following chunk generates the vegc plot.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>benchmark_obj<span class="sc">$</span>TimeAvgMap<span class="sc">$</span><span class="fu">plot</span>()[[<span class="dv">1</span>]] <span class="co"># vegc is the first output defined in the settings</span></span></code></pre></div>
<p>With the exception of the table, all plots generated in the benchmarking are ggplot objects, which enables modifications of all visual plot elements. For example a different title can be added to the litter carbon plot with the following code.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>vegc_map <span class="ot">&lt;-</span> benchmark_obj<span class="sc">$</span>TimeAvgMap<span class="sc">$</span><span class="fu">plot</span>()[[<span class="dv">3</span>]] <span class="co"># litc is the third output of the settings</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>vegc_map <span class="sc">+</span> ggplot2<span class="sc">::</span><span class="fu">ggtitle</span>(<span class="st">&quot;Difference in litter carbon; lpjml 5.8.6 - lpjml 5.7.1&quot;</span>)</span></code></pre></div>
<p>Maybe the preceding discussion convinced you that working with the benchmark object is fun and useful. It provides a lot of interfaces to further processing steps and functionality. It may thus be valuable to save the object for later analysis. This last step of the tutorial can be done for example with</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="fu">saveRDS</span>(benchmark_obj, <span class="at">file =</span> <span class="st">&quot;benchmark_obj_vignette.rds&quot;</span>)</span></code></pre></div>
<p>This file can also be used to recreate the benchmark report. See <code>?create_pdf_report</code> for details. Don’t forget to restore the default package settings with</p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set_lpjmlstats_settings</span>(<span class="at">year_subset =</span> <span class="cn">NULL</span>)</span></code></pre></div>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
