<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />

<meta name="viewport" content="width=device-width, initial-scale=1" />



<title>benchmark</title>

<script>// Pandoc 2.9 adds attributes on both header and div. We remove the former (to
// be compatible with the behavior of Pandoc < 2.8).
document.addEventListener('DOMContentLoaded', function(e) {
  var hs = document.querySelectorAll("div.section[class*='level'] > :first-child");
  var i, h, a;
  for (i = 0; i < hs.length; i++) {
    h = hs[i];
    if (!/^h[1-6]$/i.test(h.tagName)) continue;  // it should be a header h1-h6
    a = h.attributes;
    while (a.length > 0) h.removeAttribute(a[0].name);
  }
});
</script>

<style type="text/css">
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
span.underline{text-decoration: underline;}
div.column{display: inline-block; vertical-align: top; width: 50%;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
</style>



<style type="text/css">
code {
white-space: pre;
}
.sourceCode {
overflow: visible;
}
</style>
<style type="text/css" data-origin="pandoc">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
{ counter-reset: source-line 0; }
pre.numberSource code > span
{ position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
{ content: counter(source-line);
position: relative; left: -1em; text-align: right; vertical-align: baseline;
border: none; display: inline-block;
-webkit-touch-callout: none; -webkit-user-select: none;
-khtml-user-select: none; -moz-user-select: none;
-ms-user-select: none; user-select: none;
padding: 0 4px; width: 4em;
color: #aaaaaa;
}
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa; padding-left: 4px; }
div.sourceCode
{ }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } 
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.at { color: #7d9029; } 
code span.bn { color: #40a070; } 
code span.bu { color: #008000; } 
code span.cf { color: #007020; font-weight: bold; } 
code span.ch { color: #4070a0; } 
code span.cn { color: #880000; } 
code span.co { color: #60a0b0; font-style: italic; } 
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.do { color: #ba2121; font-style: italic; } 
code span.dt { color: #902000; } 
code span.dv { color: #40a070; } 
code span.er { color: #ff0000; font-weight: bold; } 
code span.ex { } 
code span.fl { color: #40a070; } 
code span.fu { color: #06287e; } 
code span.im { color: #008000; font-weight: bold; } 
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } 
code span.kw { color: #007020; font-weight: bold; } 
code span.op { color: #666666; } 
code span.ot { color: #007020; } 
code span.pp { color: #bc7a00; } 
code span.sc { color: #4070a0; } 
code span.ss { color: #bb6688; } 
code span.st { color: #4070a0; } 
code span.va { color: #19177c; } 
code span.vs { color: #4070a0; } 
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } 
</style>
<script>
// apply pandoc div.sourceCode style to pre.sourceCode instead
(function() {
  var sheets = document.styleSheets;
  for (var i = 0; i < sheets.length; i++) {
    if (sheets[i].ownerNode.dataset["origin"] !== "pandoc") continue;
    try { var rules = sheets[i].cssRules; } catch (e) { continue; }
    var j = 0;
    while (j < rules.length) {
      var rule = rules[j];
      // check if there is a div.sourceCode rule
      if (rule.type !== rule.STYLE_RULE || rule.selectorText !== "div.sourceCode") {
        j++;
        continue;
      }
      var style = rule.style.cssText;
      // check if color or background-color is set
      if (rule.style.color === '' && rule.style.backgroundColor === '') {
        j++;
        continue;
      }
      // replace div.sourceCode by a pre.sourceCode rule
      sheets[i].deleteRule(j);
      sheets[i].insertRule('pre.sourceCode{' + style + '}', j);
    }
  }
})();
</script>




<style type="text/css">body {
background-color: #fff;
margin: 1em auto;
max-width: 700px;
overflow: visible;
padding-left: 2em;
padding-right: 2em;
font-family: "Open Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
font-size: 14px;
line-height: 1.35;
}
#TOC {
clear: both;
margin: 0 0 10px 10px;
padding: 4px;
width: 400px;
border: 1px solid #CCCCCC;
border-radius: 5px;
background-color: #f6f6f6;
font-size: 13px;
line-height: 1.3;
}
#TOC .toctitle {
font-weight: bold;
font-size: 15px;
margin-left: 5px;
}
#TOC ul {
padding-left: 40px;
margin-left: -1.5em;
margin-top: 5px;
margin-bottom: 5px;
}
#TOC ul ul {
margin-left: -2em;
}
#TOC li {
line-height: 16px;
}
table {
margin: 1em auto;
border-width: 1px;
border-color: #DDDDDD;
border-style: outset;
border-collapse: collapse;
}
table th {
border-width: 2px;
padding: 5px;
border-style: inset;
}
table td {
border-width: 1px;
border-style: inset;
line-height: 18px;
padding: 5px 5px;
}
table, table th, table td {
border-left-style: none;
border-right-style: none;
}
table thead, table tr.even {
background-color: #f7f7f7;
}
p {
margin: 0.5em 0;
}
blockquote {
background-color: #f6f6f6;
padding: 0.25em 0.75em;
}
hr {
border-style: solid;
border: none;
border-top: 1px solid #777;
margin: 28px 0;
}
dl {
margin-left: 0;
}
dl dd {
margin-bottom: 13px;
margin-left: 13px;
}
dl dt {
font-weight: bold;
}
ul {
margin-top: 0;
}
ul li {
list-style: circle outside;
}
ul ul {
margin-bottom: 0;
}
pre, code {
background-color: #f7f7f7;
border-radius: 3px;
color: #333;
white-space: pre-wrap; 
}
pre {
border-radius: 3px;
margin: 5px 0px 10px 0px;
padding: 10px;
}
pre:not([class]) {
background-color: #f7f7f7;
}
code {
font-family: Consolas, Monaco, 'Courier New', monospace;
font-size: 85%;
}
p > code, li > code {
padding: 2px 0px;
}
div.figure {
text-align: center;
}
img {
background-color: #FFFFFF;
padding: 2px;
border: 1px solid #DDDDDD;
border-radius: 3px;
border: 1px solid #CCCCCC;
margin: 0 5px;
}
h1 {
margin-top: 0;
font-size: 35px;
line-height: 40px;
}
h2 {
border-bottom: 4px solid #f7f7f7;
padding-top: 10px;
padding-bottom: 2px;
font-size: 145%;
}
h3 {
border-bottom: 2px solid #f7f7f7;
padding-top: 10px;
font-size: 120%;
}
h4 {
border-bottom: 1px solid #f7f7f7;
margin-left: 8px;
font-size: 105%;
}
h5, h6 {
border-bottom: 1px solid #ccc;
font-size: 105%;
}
a {
color: #0033dd;
text-decoration: none;
}
a:hover {
color: #6666ff; }
a:visited {
color: #800080; }
a:visited:hover {
color: #BB00BB; }
a[href^="http:"] {
text-decoration: underline; }
a[href^="https:"] {
text-decoration: underline; }

code > span.kw { color: #555; font-weight: bold; } 
code > span.dt { color: #902000; } 
code > span.dv { color: #40a070; } 
code > span.bn { color: #d14; } 
code > span.fl { color: #d14; } 
code > span.ch { color: #d14; } 
code > span.st { color: #d14; } 
code > span.co { color: #888888; font-style: italic; } 
code > span.ot { color: #007020; } 
code > span.al { color: #ff0000; font-weight: bold; } 
code > span.fu { color: #900; font-weight: bold; } 
code > span.er { color: #a61717; background-color: #e3d2d2; } 
</style>




</head>

<body>




<h1 class="title toc-ignore">benchmark</h1>



<p>This vignette guides you through using the benchmark function of
lpjmlstats in a typical LPJmL development workflow.</p>
<p>Say you have been working on a change in the LPJmL setup, tweaking
some settings in the config or making changes to the source code. A
global run with the new setup has been performed and the output was
sucessfully written to the output directory. At this point, the natural
question arises: ‘How have the modifications affected the results?’ The
benchmarking system helps find an answer by<br />
systematically comparing the outputs of the modified, currently “under
test,” LPJmL setup against the outputs of a stable “baseline” setup.</p>
<p>Results from different baseline LPJmL versions can be found in the
<code>p/projects/lpjml/benchmark_run_outputs/</code> directory on the
cluster.</p>
<p>This package is a component of the PIAM cluster module. It is
recommended to load this module via <code>module load piam</code> (on
the hpc 2024
<code>source /p/system/modulefiles/defaults/piam/module_load_piam</code>)
before starting the R session, ensuring that all necessary packages are
loaded. After loading the PIAM module you may need to temporarily
disable personal R packages to avoid conflicts using
<code>liboff</code>. Remember to re-enable them with <code>libon</code>
when finished with the benchmarking.</p>
<div id="using-the-benchmark-function" class="section level1">
<h1>Using the benchmark function</h1>
<p>In this vignette we will evaluate how the results of LPJmL changed,
going from version 5.7.1 to version 5.8.6.</p>
<p>Start by loading the package.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(lpjmlstats) <span class="co"># nolint</span></span></code></pre></div>
<p>The basic usage of the benchmarking function is simple. Enter the
paths of the under test and baseline output directories, along with some
meta-information.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" tabindex="-1"></a>benchmark_result <span class="ot">&lt;-</span></span>
<span id="cb2-2"><a href="#cb2-2" tabindex="-1"></a>  <span class="fu">benchmark</span>(</span>
<span id="cb2-3"><a href="#cb2-3" tabindex="-1"></a>    <span class="at">baseline_dir =</span> <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.7.1/output&quot;</span>,    <span class="co"># nolint</span></span>
<span id="cb2-4"><a href="#cb2-4" tabindex="-1"></a>    <span class="co"># it is recommended to use absolute paths</span></span>
<span id="cb2-5"><a href="#cb2-5" tabindex="-1"></a>    <span class="at">under_test_dirs =</span> <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.8.6/output&quot;</span>, <span class="co"># nolint</span></span>
<span id="cb2-6"><a href="#cb2-6" tabindex="-1"></a>    <span class="at">author =</span> <span class="st">&quot;David&quot;</span>,</span>
<span id="cb2-7"><a href="#cb2-7" tabindex="-1"></a>    <span class="at">description =</span> <span class="st">&quot;Evaluate the changes from version 5.7.1 to 5.8.6&quot;</span>,</span>
<span id="cb2-8"><a href="#cb2-8" tabindex="-1"></a>    <span class="at">output_file =</span> <span class="st">&quot;benchmark_vignette.pdf&quot;</span></span>
<span id="cb2-9"><a href="#cb2-9" tabindex="-1"></a>  )</span></code></pre></div>
<p>This will create a PDF report in the working directory and stores the
numerical results in the <code>benchmark_result</code> object. The
process usually takes several minutes. It will summarize and compare the
typically most relevant LPJmL outputs. Note that the console output
highlights the currently processed output variable in blue. After the
core numerical benchmarking is completed, it will start to generate the
PDF report, which visualizes the numerical results in different plots
and tables.</p>
<p>Use <code>?benchmark</code> to see how the output directory for the
PDF report as well as other function arguments can be defined. In the
details section of the function documentation you can also see the
requirements for the function to work properly.</p>
<p>One of the most important other arguments that can be passed to the
benchmark function is the <code>settings</code> R list. It defines the
full protocol of the benchmarking process, including which outputs are
considered, how they are processed, compared and visualized. If no
settings are passed, the default as given in the
<code>default_settings</code> list is used. Here is an excerpt of how
that list is defined in <code>R\default_settings.R</code>:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" tabindex="-1"></a>default_settings <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb3-2"><a href="#cb3-2" tabindex="-1"></a>  <span class="at">vegc  =</span> <span class="fu">c</span>(GlobSumTimeAvgTable, GlobSumAnnAvgTimeseries, TimeAvgMap),</span>
<span id="cb3-3"><a href="#cb3-3" tabindex="-1"></a>  <span class="at">soilc =</span> <span class="fu">c</span>(GlobSumTimeAvgTable, GlobSumAnnAvgTimeseries, TimeAvgMap),</span>
<span id="cb3-4"><a href="#cb3-4" tabindex="-1"></a>  <span class="at">litc =</span> <span class="fu">c</span>(GlobSumTimeAvgTable, GlobSumAnnAvgTimeseries, TimeAvgMap),</span>
<span id="cb3-5"><a href="#cb3-5" tabindex="-1"></a>  ...</span>
<span id="cb3-6"><a href="#cb3-6" tabindex="-1"></a>)</span></code></pre></div>
<p>The settings list reads as follows: Each line corresponds to an
output variable. The left hand side of the equality sign defines
<em>which output is processed</em> and the right hand side defines
<em>how this output is processed</em>.</p>
<p>More specifically the right hand side defines which metrics are used
for the evaluation of the output. A metric is a generic pipeline to
summarize, compare and visualize output data. It is generic because the
same metric can be applied to different outputs. The metrics are the
cornerstone of the benchmarking process. See <code>?Metric</code> for
technical details.</p>
<p>The metric <code>GlobSumTimeAvgTable</code> globally sums up all
cellular values weighted by reference area, and then averages over time.
The resulting scalar values are compared and displayed in a table. The
metric <code>GlobSumAnnAvgTimeseries</code> also computes a weighted
sum, but only averages over each year (if the output is not already
given anually). The resulting time series are displayed in a plot.
Finally, <code>TimeAvgMap</code> only averages over time, and plots the
differences from under-test to baseline as maps. Type
<code>?GlobSumTimeAvgTable</code> for details.</p>
<p>Often it is needed to change these settings, for example when new
outputs should be evaluated. Read the <a href="benchmark-change-settings.html">benchmark-change-settings</a>
vignette for a tutorial.</p>
<p>Benchmarking multiple under test runs against a baseline is also
possible. For example, it could be interesting to compare the 5.8.1
version with several older versions.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" tabindex="-1"></a><span class="fu">benchmark</span>(</span>
<span id="cb4-2"><a href="#cb4-2" tabindex="-1"></a>  <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.8.1/output&quot;</span>,   <span class="co"># nolint</span></span>
<span id="cb4-3"><a href="#cb4-3" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb4-4"><a href="#cb4-4" tabindex="-1"></a>    <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.7.7/output&quot;</span>, <span class="co"># nolint</span></span>
<span id="cb4-5"><a href="#cb4-5" tabindex="-1"></a>    <span class="co"># note that older versions than 5.7.1 can at the moment not be benchmarked due</span></span>
<span id="cb4-6"><a href="#cb4-6" tabindex="-1"></a>    <span class="co"># to missing terr_area output</span></span>
<span id="cb4-7"><a href="#cb4-7" tabindex="-1"></a>    <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.7.1/output&quot;</span>  <span class="co"># nolint</span></span>
<span id="cb4-8"><a href="#cb4-8" tabindex="-1"></a>  ),</span>
<span id="cb4-9"><a href="#cb4-9" tabindex="-1"></a>  <span class="at">author =</span> <span class="st">&quot;David&quot;</span>,</span>
<span id="cb4-10"><a href="#cb4-10" tabindex="-1"></a>  <span class="at">description =</span> <span class="st">&quot;Check how newer lpjml versions compare to older versions&quot;</span>,</span>
<span id="cb4-11"><a href="#cb4-11" tabindex="-1"></a>  <span class="at">output_file =</span> <span class="st">&quot;benchmark_vignette_multiple_ut.pdf&quot;</span></span>
<span id="cb4-12"><a href="#cb4-12" tabindex="-1"></a>)</span></code></pre></div>
<p>Comparing more than two under test to the baseline may result in
display problems in the PDF report.</p>
<p>You can also give short identifiers for the simulation that will be
used throughout at this point:</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="#cb5-1" tabindex="-1"></a><span class="fu">benchmark</span>(</span>
<span id="cb5-2"><a href="#cb5-2" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">my_baseline =</span> <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.8.1/output&quot;</span>),   <span class="co"># nolint</span></span>
<span id="cb5-3"><a href="#cb5-3" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb5-4"><a href="#cb5-4" tabindex="-1"></a>    <span class="at">my_test1 =</span> <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.7.7/output&quot;</span>, <span class="co"># nolint</span></span>
<span id="cb5-5"><a href="#cb5-5" tabindex="-1"></a>    <span class="co"># note that older versions than 5.7.1 can at the moment not be benchmarked due</span></span>
<span id="cb5-6"><a href="#cb5-6" tabindex="-1"></a>    <span class="co"># to missing terr_area output</span></span>
<span id="cb5-7"><a href="#cb5-7" tabindex="-1"></a>    <span class="at">my_test2 =</span> <span class="st">&quot;/p/projects/lpjml/benchmark_run_outputs/5.7.1/output&quot;</span>  <span class="co"># nolint</span></span>
<span id="cb5-8"><a href="#cb5-8" tabindex="-1"></a>  ),</span>
<span id="cb5-9"><a href="#cb5-9" tabindex="-1"></a>  ...</span>
<span id="cb5-10"><a href="#cb5-10" tabindex="-1"></a>)</span></code></pre></div>
</div>
<div id="working-with-the-benchmark-results" class="section level1">
<h1>Working with the benchmark results</h1>
<div id="the-pdf-report" class="section level2">
<h2>The PDF report</h2>
<p>The PDF report offers a visualization of the numerical benchmarking
results. It is structured by the different metrics used in the settings.
Each metric has its own section where the result from all outputs using
this metric are displayed.</p>
<p>The front page shows some meta information on the benchmarking. Of
particular importance is the simulation table, that maps short
simulation identifiers to the runs considered in the benchmarking.
Usually these identifier are abbreviations of the simulation names, as
defined in the lpjml config. If these names are not unique,
abbreviations of the simulation paths are used. The identifiers are
employed because the simulation names and paths are often quite long,
making it more convenient to use short abbreviations to refer to the
different runs in the report, especially when comparing the baseline
with multiple under test runs.</p>
</div>
<div id="the-benchmark-result-object" class="section level2">
<h2>The benchmark result object</h2>
<p>The benchmarking function also outputs its numerical results as a
benchmark result object. Accessing this data is a bit technical, so skip
this section if you are only interested in the PDF report.</p>
<p>The benchmark result behaves similarly to a nested R list. The top
layer is again structured by the different metrics. The data that is
generated by a metric is stored in its var_grp_list attribute. It
contains the results for all the variables that the metric evaluated as
a list of so called variable groups.<br />
Each variable group consists of the processed baseline and (possibly
multiple) under test outputs of that variable, as well as the
comparisons of all under test outputs against the baseline. See
<code>?Metric</code> for details on how the metrics organize their
processed data.</p>
<p>Say we are interested in how the time series of global vegetation
carbon has changed in the newer version. The data from the baseline run
can be accessed via</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" tabindex="-1"></a>vegc_old_ts <span class="ot">&lt;-</span> benchmark_result<span class="sc">$</span>GlobSumAnnAvgTimeseries<span class="sc">$</span>var_grp_list<span class="sc">$</span>vegc<span class="sc">$</span>baseline</span>
<span id="cb6-2"><a href="#cb6-2" tabindex="-1"></a>vegc_old_ts</span></code></pre></div>
<p>Note that this is an LPJmLData object of <code>lpjmlkit</code>.
Evaluating an LPJmLData object in the R Console will provide you with a
lot of useful meta data, about the origin and processing history of the
object. Note that subset is <code>TRUE</code> and space aggregation is
<code>weighted_sum</code>, which reflects the processing steps done by
the benchmarking function.</p>
<p>You will encounter several elements of the <code>lpjmlkit</code>
package in <code>lpjmlstats</code> - in fact <code>lpjmlstats</code>
completly builds upon <code>lpjmlkit</code>.</p>
<p>This also means that we can employ the functionality of this latter
package to work with the benchmark results. For example, to output the
first 5 years of <code>vegc_old_ts</code> we use</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="#cb7-1" tabindex="-1"></a><span class="fu">subset</span>(vegc_old_ts, <span class="at">time =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)<span class="sc">$</span>data</span></code></pre></div>
<p>For under test results the simulation identifiers are used to access
the data. This is needed to differentiate the under test outputs if
multiple runs are benchmarked against a baseline run.</p>
<p>The simulation table that relates the identifiers to simulation
names, as well as other meta data can be accessed via:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="#cb8-1" tabindex="-1"></a><span class="fu">get_benchmark_meta_data</span>(benchmark_result)</span></code></pre></div>
<p>Note that the simulation with the new version has the identifier
<code>5.8.</code>. Thus, to access the new vegc time series of the new
LPJmL version we use</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" tabindex="-1"></a>vegc_new_ts <span class="ot">&lt;-</span> benchmark_result<span class="sc">$</span>GlobSumAnnAvgTimeseries<span class="sc">$</span>var_grp_list<span class="sc">$</span>vegc<span class="sc">$</span>under_test<span class="sc">$</span><span class="st">`</span><span class="at">5.8.</span><span class="st">`</span></span></code></pre></div>
<p>To check the difference between the two time series we use</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb10-1"><a href="#cb10-1" tabindex="-1"></a><span class="fu">subset</span>(vegc_new_ts, <span class="at">time =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)<span class="sc">$</span>data <span class="sc">-</span> <span class="fu">subset</span>(vegc_old_ts, <span class="at">time =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)<span class="sc">$</span>data</span></code></pre></div>
<p>Observe that the newer version produced less vegetation carbon in the
first five years.</p>
<p>The data from the benchmarking object in combination with the
functionality of lpjmlkit is also useful to make comparisons to
literature data. Say we have a reference dataset of global soil nitrogen
given as a terra object. Again the tools from lpjmlkit come in handy to
convert the benchmark data to terra:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="#cb11-1" tabindex="-1"></a>soiln_lpjmldata <span class="ot">&lt;-</span> benchmark_result<span class="sc">$</span>TimeAvgMap<span class="sc">$</span>var_grp_list<span class="sc">$</span>soiln<span class="sc">$</span>under_test<span class="sc">$</span><span class="st">`</span><span class="at">5.8.</span><span class="st">`</span></span>
<span id="cb11-2"><a href="#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" tabindex="-1"></a>soiln_terra <span class="ot">&lt;-</span> lpjmlkit<span class="sc">::</span><span class="fu">as_terra</span>(soiln_lpjmldata)</span>
<span id="cb11-4"><a href="#cb11-4" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" tabindex="-1"></a>soiln_terra</span>
<span id="cb11-6"><a href="#cb11-6" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" tabindex="-1"></a><span class="co"># read in validation data terra</span></span>
<span id="cb11-8"><a href="#cb11-8" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" tabindex="-1"></a><span class="co"># ...</span></span></code></pre></div>
<p>Another convenient feature of the benchmark data object is that all
plot functions used for the report generation can also be called
individually from here. For example the table of the
<code>GlobSumTimeAvgTable</code> metric can be generated via</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="#cb12-1" tabindex="-1"></a>benchmark_result<span class="sc">$</span>GlobSumTimeAvgTable<span class="sc">$</span><span class="fu">plot</span>()</span></code></pre></div>
<p>This enables a terminal-only benchmarking without generating plots or
reports. See <code>?benchmark</code> for how to skip the PDF report
generation in the benchmarking.</p>
<p>Sometimes individual plots of the benchmarking may be needed in
another context. The following chunk generates the vegc plot.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="#cb13-1" tabindex="-1"></a>benchmark_result<span class="sc">$</span>TimeAvgMap<span class="sc">$</span><span class="fu">plot</span>()[[<span class="dv">1</span>]] <span class="co"># vegc is the first output defined in the settings</span></span></code></pre></div>
<p>With the exception of the table, all plots generated in the
benchmarking are ggplot objects, which enables modifications of all
visual plot elements. For example a different title can be added to the
litter carbon plot with the following code.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="#cb14-1" tabindex="-1"></a>vegc_map <span class="ot">&lt;-</span> benchmark_result<span class="sc">$</span>TimeAvgMap<span class="sc">$</span><span class="fu">plot</span>()[[<span class="dv">3</span>]] <span class="co"># litc is the third output of the settings</span></span>
<span id="cb14-2"><a href="#cb14-2" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" tabindex="-1"></a>vegc_map <span class="sc">+</span> ggplot2<span class="sc">::</span><span class="fu">ggtitle</span>(<span class="st">&quot;Difference in litter carbon; lpjml 5.8.6 - lpjml 5.7.1&quot;</span>)</span></code></pre></div>
<p>Maybe the preceding discussion convinced you that working with the
benchmark object is fun and useful. It provides a lot of interfaces to
further processing steps and functionality. It may thus be valuable to
save the object for later analysis. This last step of the tutorial can
be done for example with</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="#cb15-1" tabindex="-1"></a><span class="fu">saveRDS</span>(benchmark_result, <span class="at">file =</span> <span class="st">&quot;benchmark_result_vignette.rds&quot;</span>)</span></code></pre></div>
<p>This file can also be used to recreate the benchmark report. See
<code>?create_pdf_report</code> for details.</p>
</div>
</div>



<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
